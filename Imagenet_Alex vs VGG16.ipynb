{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "# add relprop() method to each layer\n",
    "########################################\n",
    "class Linear(nn.Linear):\n",
    "    def __init__(self, linear):\n",
    "        super(nn.Linear, self).__init__()\n",
    "        self.in_features = linear.in_features\n",
    "        self.out_features = linear.out_features\n",
    "        self.weight = linear.weight\n",
    "        self.bias = linear.bias\n",
    "        \n",
    "    def relprop(self, R):\n",
    "        V = torch.clamp(self.weight, min=0)\n",
    "        Z = torch.mm(self.X, torch.transpose(V,0,1)) + 1e-9\n",
    "        S = R / Z\n",
    "        C = torch.mm(S, V)\n",
    "        R = self.X * C\n",
    "        return R\n",
    "        \n",
    "class ReLU(nn.ReLU):   \n",
    "    def relprop(self, R): \n",
    "        return R\n",
    "\n",
    "class Reshape_Alex(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Reshape_Alex, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 256*6*6)\n",
    "        \n",
    "    def relprop(self, R):\n",
    "        return R.view(-1, 256, 6, 6)\n",
    "\n",
    "class Reshape_VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Reshape_VGG, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 512*7*7)\n",
    "        \n",
    "    def relprop(self, R):\n",
    "        return R.view(-1, 512, 7, 7)\n",
    "\n",
    "class MaxPool2d(nn.MaxPool2d):\n",
    "    def __init__(self, maxpool2d):\n",
    "        super(nn.MaxPool2d, self).__init__()\n",
    "        self.kernel_size = maxpool2d.kernel_size\n",
    "        self.stride = maxpool2d.stride\n",
    "        self.padding = maxpool2d.padding\n",
    "        self.dilation = maxpool2d.dilation\n",
    "        self.return_indices = maxpool2d.return_indices\n",
    "        self.ceil_mode = maxpool2d.ceil_mode\n",
    "        \n",
    "    def gradprop(self, DY):\n",
    "        DX = self.X * 0\n",
    "        temp, indices = F.max_pool2d(self.X, self.kernel_size, self.stride, \n",
    "                                     self.padding, self.dilation, self.ceil_mode, True)\n",
    "        DX = F.max_unpool2d(DY, indices, self.kernel_size, self.stride, self.padding)\n",
    "        return DX\n",
    "    \n",
    "    def relprop(self, R):\n",
    "        Z = self.Y + 1e-9\n",
    "        S = R / Z\n",
    "        C = self.gradprop(S)\n",
    "        R = self.X * C\n",
    "        return R\n",
    "\n",
    "class Conv2d(nn.Conv2d):\n",
    "    def __init__(self, conv2d):\n",
    "        super(nn.Conv2d, self).__init__(conv2d.in_channels, \n",
    "                                        conv2d.out_channels, \n",
    "                                        conv2d.kernel_size, \n",
    "                                        conv2d.stride, \n",
    "                                        conv2d.padding, \n",
    "                                        conv2d.dilation, \n",
    "                                        conv2d.transposed, \n",
    "                                        conv2d.output_padding, \n",
    "                                        conv2d.groups, \n",
    "                                        True)\n",
    "        self.weight = conv2d.weight\n",
    "        self.bias = conv2d.bias\n",
    "        \n",
    "    def gradprop(self, DY):\n",
    "        output_padding = self.X.size()[2] - ((self.Y.size()[2] - 1) * self.stride[0] \\\n",
    "                                             - 2 * self.padding[0] + self.kernel_size[0])\n",
    "        return F.conv_transpose2d(DY, self.weight, stride=self.stride, \n",
    "                                  padding=self.padding, output_padding=output_padding)\n",
    "        \n",
    "    def relprop(self, R):\n",
    "        Z = self.Y + 1e-9\n",
    "        S = R / Z\n",
    "        C = self.gradprop(S)\n",
    "        R = self.X * C\n",
    "        return R\n",
    "########################################\n",
    "\n",
    "# hyperparameters\n",
    "num_workers = 4\n",
    "batch_size = 10\n",
    "# buffer to store predictions & labels\n",
    "buffer_label = list()\n",
    "buffer_Alex = list()\n",
    "buffer_VGG = list()\n",
    "\n",
    "# directory for input images\n",
    "val_dir = 'ILSVRC2012_img_val'\n",
    "# directory for output heatmaps\n",
    "out_dir = 'LRP'\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "# define data loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(val_dir,\n",
    "                         transforms.Compose([\n",
    "                             transforms.Scale(256),\n",
    "                             transforms.CenterCrop(224),\n",
    "                             transforms.ToTensor(),\n",
    "                             normalize,\n",
    "                         ])),\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# load pre-trained Alexnet\n",
    "alex = models.alexnet(pretrained=True).cuda()\n",
    "for param in alex.parameters():\n",
    "    param.requires_grad = False\n",
    "# load pre-trained VGG16\n",
    "vgg16 = models.vgg16(pretrained=True).cuda()\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            Conv2d(alex.features[0]),\n",
    "            ReLU(),\n",
    "            MaxPool2d(alex.features[2]),\n",
    "            Conv2d(alex.features[3]),\n",
    "            ReLU(),\n",
    "            MaxPool2d(alex.features[5]),\n",
    "            Conv2d(alex.features[6]),\n",
    "            ReLU(),\n",
    "            Conv2d(alex.features[8]),\n",
    "            ReLU(),\n",
    "            Conv2d(alex.features[10]),\n",
    "            ReLU(),\n",
    "            MaxPool2d(alex.features[12]),\n",
    "            Reshape_Alex(),\n",
    "            Linear(alex.classifier[1]),\n",
    "            ReLU(),\n",
    "            Linear(alex.classifier[4]),\n",
    "            ReLU(),\n",
    "            Linear(alex.classifier[6])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "        \n",
    "    def relprop(self, R):\n",
    "        for l in range(len(self.layers), 0, -1):\n",
    "            R = self.layers[l-1].relprop(R)\n",
    "        return R\n",
    "\n",
    "class VGG16Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Net, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            Conv2d(vgg16.features[0]),\n",
    "            ReLU(),\n",
    "            Conv2d(vgg16.features[2]),\n",
    "            ReLU(),\n",
    "            MaxPool2d(vgg16.features[4]),\n",
    "            Conv2d(vgg16.features[5]),\n",
    "            ReLU(),\n",
    "            Conv2d(vgg16.features[7]),\n",
    "            ReLU(),\n",
    "            MaxPool2d(vgg16.features[9]),\n",
    "            Conv2d(vgg16.features[10]),\n",
    "            ReLU(),\n",
    "            Conv2d(vgg16.features[12]),\n",
    "            ReLU(),\n",
    "            Conv2d(vgg16.features[14]),\n",
    "            ReLU(),\n",
    "            MaxPool2d(vgg16.features[16]),\n",
    "            Conv2d(vgg16.features[17]),\n",
    "            ReLU(),\n",
    "            Conv2d(vgg16.features[19]),\n",
    "            ReLU(),\n",
    "            Conv2d(vgg16.features[21]),\n",
    "            ReLU(),\n",
    "            MaxPool2d(vgg16.features[23]),\n",
    "            Conv2d(vgg16.features[24]),\n",
    "            ReLU(),\n",
    "            Conv2d(vgg16.features[26]),\n",
    "            ReLU(),\n",
    "            Conv2d(vgg16.features[28]),\n",
    "            ReLU(),\n",
    "            MaxPool2d(vgg16.features[30]),\n",
    "            Reshape_VGG(),\n",
    "            Linear(vgg16.classifier[0]),\n",
    "            ReLU(),\n",
    "            Linear(vgg16.classifier[3]),\n",
    "            ReLU(),\n",
    "            Linear(vgg16.classifier[6])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "        \n",
    "    def relprop(self, R):\n",
    "        for l in range(len(self.layers), 0, -1):\n",
    "            R = self.layers[l-1].relprop(R)\n",
    "        return R\n",
    "\n",
    "model_Alex = AlexNet().cuda()\n",
    "model_VGG = VGG16Net().cuda()\n",
    "\n",
    "# forward hook method for retrieving intermediate results\n",
    "def forward_hook(self, input, output):\n",
    "    self.X = input[0]\n",
    "    self.Y = output\n",
    "    \n",
    "for i in range(0, len(model_Alex.layers)):\n",
    "    model_Alex.layers[i].register_forward_hook(forward_hook)\n",
    "    \n",
    "for i in range(0, len(model_VGG.layers)):\n",
    "    model_VGG.layers[i].register_forward_hook(forward_hook)\n",
    "\n",
    "model_Alex.eval()\n",
    "model_VGG.eval()\n",
    "\n",
    "correct_Alex = 0\n",
    "correct_VGG = 0\n",
    "buffer_label = []\n",
    "buffer_Alex = []\n",
    "buffer_VGG = []\n",
    "\n",
    "for idx, (input, label) in enumerate(val_loader):\n",
    "    input, label = Variable(input, volatile=True).cuda(), Variable(label).cuda()\n",
    "    \n",
    "    output_Alex = model_Alex(input)\n",
    "    pred_Alex = output_Alex.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct_Alex += pred_Alex.eq(label.data.view_as(pred_Alex)).cpu().sum()\n",
    "    \n",
    "    T_Alex = pred_Alex.squeeze().cpu().numpy()\n",
    "    T_Alex = (T_Alex[:,np.newaxis] == np.arange(1000))*1.0\n",
    "    T_Alex = torch.from_numpy(T_Alex).type(torch.FloatTensor)\n",
    "    T_Alex = Variable(T_Alex).cuda()\n",
    "    LRP_Alex = model_Alex.relprop(output_Alex * T_Alex)\n",
    "    \n",
    "    output_VGG = model_VGG(input)\n",
    "    pred_VGG = output_VGG.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "    correct_VGG += pred_VGG.eq(label.data.view_as(pred_VGG)).cpu().sum()\n",
    "    \n",
    "    T_VGG = pred_VGG.squeeze().cpu().numpy()\n",
    "    T_VGG = (T_VGG[:,np.newaxis] == np.arange(1000))*1.0\n",
    "    T_VGG = torch.from_numpy(T_VGG).type(torch.FloatTensor)\n",
    "    T_VGG = Variable(T_VGG).cuda()\n",
    "    LRP_VGG = model_VGG.relprop(output_VGG * T_VGG)\n",
    "    \n",
    "    buffer_label.append(label.data.cpu().numpy())\n",
    "    buffer_Alex.append(pred_Alex.cpu().numpy())\n",
    "    buffer_VGG.append(pred_VGG.cpu().numpy())\n",
    "    \n",
    "    # save results which are classified correctly by VGG16, incorrectly by AlexNet\n",
    "    for i in range(0, batch_size):\n",
    "        if (pred_Alex.squeeze().cpu().numpy()[i] != label.data.cpu().numpy()[i]) \\\n",
    "        and (pred_VGG.squeeze().cpu().numpy()[i] == label.data.cpu().numpy()[i]):\n",
    "            img = input[i].permute(1,2,0).data.cpu().numpy()\n",
    "            img = 255 * (img-img.min()) / (img.max()-img.min())\n",
    "            img = img.astype('uint8')\n",
    "            Image.fromarray(img, 'RGB').save(directory + '/%d_input_%d.JPEG' \\\n",
    "                                             % ((idx*batch_size+i, label.data.cpu().numpy()[i])))\n",
    "                    \n",
    "            heatmap_Alex = LRP_Alex[i].permute(1,2,0).data.cpu().numpy()\n",
    "            heatmap_Alex = np.absolute(heatmap_Alex)\n",
    "            heatmap_Alex = 255 * (heatmap_Alex-heatmap_Alex.min()) / (heatmap_Alex.max()-heatmap_Alex.min())\n",
    "            heatmap_Alex = heatmap_Alex.astype('uint8')\n",
    "            Image.fromarray(heatmap_Alex, 'RGB').save(directory + '/%d_LRP_Alex_%d.JPEG' \\\n",
    "                                                 % ((idx*batch_size+i, pred_Alex.squeeze().cpu().numpy()[i])))\n",
    "            \n",
    "            heatmap_VGG = LRP_VGG[i].permute(1,2,0).data.cpu().numpy()\n",
    "            heatmap_VGG = np.absolute(heatmap_VGG)\n",
    "            heatmap_VGG = 255 * (heatmap_VGG-heatmap_VGG.min()) / (heatmap_VGG.max()-heatmap_VGG.min())\n",
    "            heatmap_VGG = heatmap_VGG.astype('uint8')\n",
    "            Image.fromarray(heatmap_VGG, 'RGB').save(directory + '/%d_LRP_VGG_%d.JPEG' \\\n",
    "                                                 % ((idx*batch_size+i, pred_VGG.squeeze().cpu().numpy()[i])))\n",
    "\n",
    "print('Done...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
